
# ğŸ“¦ é¡¹ç›®åç§°ï¼šAIå¹¿å‘Šæ–‡æ¡ˆç”ŸæˆåŠ©æ‰‹ï¼ˆä½ä»£ç  LangChain Agent è¿›é˜¶ç‰ˆï¼‰
# ğŸ’» æŠ€æœ¯æ ˆï¼šLangChain + OpenAI + Streamlit + Python
# âœ… åŠŸèƒ½ï¼šå¯¹è¯å¼å¹¿å‘Šæ–‡æ¡ˆç”Ÿæˆï¼Œæ”¯æŒè¯­æ°”é€‰æ‹©ã€å¤šè¯­è¨€ã€æœ¬åœ°å“ç‰ŒçŸ¥è¯†æ¥å…¥ã€è®°å¿†åŠŸèƒ½ã€å¤šè½®äº¤äº’

import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.agents import initialize_agent, Tool
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain.chains.combine_documents.stuff import StuffDocumentsChain
from langchain.vectorstores import FAISS
from langchain.document_loaders import TextLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains.retrieval_qa.base import RetrievalQA
import os
import tempfile
import shutil

# å®‰å…¨æ–¹å¼è¯»å– API å¯†é’¥
openai_api_key = os.getenv("OPENAI_API_KEY", st.secrets.get("openai_api_key", ""))
os.environ["OPENAI_API_KEY"] = openai_api_key

# åˆå§‹åŒ–LLMå’Œè®°å¿†
llm = ChatOpenAI(temperature=0.7)
memory = ConversationBufferMemory(memory_key="chat_history")

# Streamlitç•Œé¢
st.title("ğŸ¯ AIå¹¿å‘Šæ–‡æ¡ˆç”ŸæˆåŠ©æ‰‹ï¼ˆè¿›é˜¶ç‰ˆï¼‰")
st.markdown("ä¸Šä¼ å“ç‰Œèµ„æ–™ + å¤šè½®äº¤äº’å¼è¾“å…¥ + å¤šè¯­è¨€æ–‡æ¡ˆè¾“å‡º")

# ä¸Šä¼ å“ç‰Œæ–‡æ¡£
uploaded_file = st.file_uploader("ğŸ“„ ä¸Šä¼ å“ç‰Œè°ƒæ€§æ–‡æ¡£ï¼ˆtxtæ ¼å¼ï¼‰", type=["txt"])

if uploaded_file:
    with tempfile.NamedTemporaryFile(delete=False) as tmp_file:
        tmp_file.write(uploaded_file.read())
        tmp_path = tmp_file.name

    loader = TextLoader(tmp_path)
    docs = loader.load()

    # æ„å»ºå‘é‡ç´¢å¼•åº“
    embeddings = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(docs, embeddings)

    # åŸºäºRAGæ„å»ºæ£€ç´¢å¢å¼ºç”Ÿæˆé“¾
    retriever = vectorstore.as_retriever()
    rag_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
else:
    rag_chain = None

# äº¤äº’è¾“å…¥æ¡†
if 'product' not in st.session_state:
    st.session_state.product = st.text_input("äº§å“åç§°")
    st.stop()

if 'features' not in st.session_state:
    st.session_state.features = st.text_area("äº§å“å–ç‚¹")
    st.stop()

if 'audience' not in st.session_state:
    st.session_state.audience = st.text_input("ç›®æ ‡äººç¾¤")
    st.stop()

platform = st.selectbox("æŠ•æ”¾å¹³å°", ["å°çº¢ä¹¦", "TikTok", "Instagram", "å¾®åš"])
tone = st.selectbox("è¯­æ°”é£æ ¼", ["æƒ…ç»ªåŒ–", "ç†æ€§è¯´æœ", "å¹½é»˜", "æƒå¨"])
language = st.selectbox("è¾“å‡ºè¯­è¨€", ["ä¸­æ–‡", "è‹±æ–‡", "æ—¥æ–‡"])

# Prompt æ¨¡æ¿
template = '''
ä½ æ˜¯ä¸€ä½{platform}å¹¿å‘Šä¸“å®¶ï¼Œè¯·ä¸ºä»¥ä¸‹äº§å“ç”Ÿæˆç¬¦åˆ{tone}é£æ ¼çš„æ–‡æ¡ˆã€‚

äº§å“ï¼š{product}
å–ç‚¹ï¼š{features}
å—ä¼—ï¼š{audience}
è¯­è¨€ï¼š{language}

å“ç‰ŒèƒŒæ™¯ï¼ˆè‹¥æœ‰ï¼‰ï¼š{brand_context}

è¯·è¾“å‡º80å­—ä»¥å†…å¹¿å‘Šæ–‡æ¡ˆã€‚
'''

prompt = PromptTemplate(
    input_variables=["platform", "tone", "product", "features", "audience", "language", "brand_context"],
    template=template,
)

chain = LLMChain(llm=llm, prompt=prompt, memory=memory)

# æŒ‰é’®è§¦å‘ç”Ÿæˆ
if st.button("âœ¨ ç”Ÿæˆå¹¿å‘Šæ–‡æ¡ˆ"):
    brand_context = ""
    if rag_chain:
        brand_context = rag_chain.run("è¯·æ€»ç»“è¯¥å“ç‰Œçš„æ ¸å¿ƒè¯­è°ƒä¸å…³é”®è¯")

    inputs = {
        "platform": platform,
        "tone": tone,
        "product": st.session_state.product,
        "features": st.session_state.features,
        "audience": st.session_state.audience,
        "language": language,
        "brand_context": brand_context
    }

    with st.spinner("ç”Ÿæˆä¸­..."):
        result = chain.run(inputs)
        st.success("âœ… æ–‡æ¡ˆå·²ç”Ÿæˆï¼š")
        st.markdown(f"**{result}**")

# æ˜¾ç¤ºèŠå¤©å†å²
with st.expander("ğŸ§¾ äº¤äº’å†å²è®°å½•"):
    st.write(memory.buffer)

st.caption("ç”± LangChain + OpenAI + RAG + Memory é©±åŠ¨")
